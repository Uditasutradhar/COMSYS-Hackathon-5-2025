# COMSYS-Hackathon-5-2025
<br>
Team Name: Little Bits <br>
AUTHOR Udita SUTRADHAR (Leader) <br>
      Arpita Shaw (Member) <br>
      Dr. Showmik Bhowmik (Supervisor)
<br>

Institute: Ghani Khan Chowdhury Institute of Engineering and Technology, Malda

<br>

#Task A 
##ğŸš€ Gender Classification 
<br>
This project classifies gender (male/female) from facial images using the Vision Transformer (`ViT-base-patch16-224-in21k`) fine-tuned on a custom dataset.

---
- ğŸ”¢ Type: Binary Classification
- ğŸ§¾ Dataset:
  - train: contains 'male' and 'female' folders
  - val: follow the same format
- ğŸ Objective: Predict gender from unseen face images

---

## ğŸ—ï¸ Model

- Pretrained `ViT-Base-Patch16-224-IN21K`
- Fine-tuned using AdamW optimizer with:
  - LR: 2e-5
  - Label Smoothing: 0.1
  - Augmentations: RandomResizedCrop, ColorJitter, HorizontalFlip

---

## ğŸƒ Training Pipeline

- âœ… Checkpointing per epoch
- âœ… Best model saved as 'best_finetuned'
- âœ… Metrics computed: Accuracy, Precision, Recall, F1-score

---

## ğŸ“Š Final Evaluation Results

On the Val set:

- âœ… Accuracy : 0.9739
- ğŸ¯ Precision: 0.9722
- ğŸ” Recall   : 0.9937
- ğŸ“Œ F1 Score : 0.9828


---

## ğŸ”¬ Explain ability

We visualize the 'attention maps' of misclassified images to interpret the modelâ€™s decisions:

![Attention Example](./attention_overlay.jpg)

> Attention maps generated using ViTâ€™s final-layer self-attention and overlayed using OpenCV.

---

## â— Error Analysis

We save all misclassified test images in `misclassified_test_images/` with filenames like:3_1_pred-female_true-male.jpg
